{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "398659c1-259c-4755-8ef8-f1de80195db3",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "\n",
    "### This notebook will;\n",
    "#### 1. Read the Delta Lake ingested files and perform transformations - final data to be analyzed using Azure Synapse\n",
    "#### 2. Save transformed data into 'ADLS Transformation Layer', again with storage format as Delta Lake & partitioned by date\n",
    "\n",
    "Note: The %run command is used to get variables/ functions that are common for both ingest and transform notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60272f22-6276-4d09-8afe-8d2b31f24bd3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./FlightData-commonconfigs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24934889-97ba-4b53-9653-185a83c67ac8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "####################################### TRANSFORMATION #######################################\n",
    "\n",
    "def transformation(storage_account_name,storage_container_name):\n",
    "\n",
    "    # Defining the schema of the dataframe\n",
    "    schema = StructType([\n",
    "            StructField(\"scheduled_time\", IntegerType(), True), \n",
    "            StructField(\"flight_no\", StringType(), True),\n",
    "            StructField(\"airport_city\", StringType(), True),\n",
    "            StructField(\"airport_code\", StringType(), True),\n",
    "            StructField(\"airline\", StringType(), True),\n",
    "            StructField(\"aircraft_type\", StringType(), True),\n",
    "            StructField(\"aircraft_registration\", StringType(), True),\n",
    "            StructField(\"actual_time\", IntegerType(), True)\n",
    "    ])\n",
    "\n",
    "    # Read delta file, partitioned by date, from ingestion ADLS layer\n",
    "    df = spark.read.format(\"delta\").load(f\"/mnt/{storage_account}/{storage_ingestion_container}/{date.today() - timedelta(days=1)}\")\n",
    "\n",
    "    # Drop rows if scheduled/ actual times or airline are NULL i.e. unknown\n",
    "    df = df.filter(df[\"scheduled_time\"].isNotNull())\n",
    "    df = df.filter(df[\"actual_time\"].isNotNull())\n",
    "    df = df.filter(df[\"airline\"].isNotNull())\n",
    "\n",
    "    # Getting time difference between actual and scheduled\n",
    "    # Delay in seconds\n",
    "    df = df.withColumn(\"delay\", df[\"actual_time\"] - df[\"scheduled_time\"])\n",
    "\n",
    "    # Convert epoch to timestamp\n",
    "    df = df.withColumn(\"scheduled_time\", F.to_timestamp(df[\"scheduled_time\"]))\n",
    "    df = df.withColumn(\"actual_time\", F.to_timestamp(df[\"actual_time\"]))\n",
    "\n",
    "    # Convert timestamp to appropriate timezone - \"Europe/Malta\"\n",
    "    df = df.withColumn(\"scheduled_time\", F.from_utc_timestamp(df[\"scheduled_time\"],\"Europe/Malta\"))\n",
    "    df = df.withColumn(\"actual_time\", F.from_utc_timestamp(df[\"actual_time\"],\"Europe/Malta\"))\n",
    "\n",
    "    # Sort by scheduled time parameter\n",
    "    df = df.sort(df[\"scheduled_time\"])\n",
    "\n",
    "    # Mount transformation container\n",
    "    mount_adls_using_sp(storage_account,storage_transformation_container)\n",
    "\n",
    "    # Write dataframe to ADLS (as Delta Lake) in 'ADLS Transformation Layer', again partitioned by date\n",
    "    df.write.format(\"delta\").save(f\"/mnt/{storage_account_name}/{storage_container_name}/{date.today() - timedelta(days=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0e688a0-b780-4843-b183-16b876988aa2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Driver code\n",
    "\n",
    "####################################### TRANSFORMATION #######################################\n",
    "\n",
    "# Transform data and write to 'ADLS Transformation Layer'\n",
    "transformation(storage_account,storage_transformation_container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0af4b2e2-2f9b-4d62-be91-53d817fd75ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "\n",
    "display(spark.read.format(\"delta\").load(f\"/mnt/{storage_account}/{storage_transformation_container}/{date.today() - timedelta(days=1)}\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "FlightData-Transformations",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
