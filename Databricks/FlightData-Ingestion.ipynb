{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1f5efa8-33f0-49ad-a0fe-e094c66da4c3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### This notebook will; \n",
    "\n",
    "#### 1. Create the JSON files for both (daily) arrivals and departures & store these files in a DBFS location\n",
    "#### 2. Read the JSON files and ingest into 'ADLS Ingestion Layer' as Delta Lake, partitioning by date each time\n",
    "\n",
    "Note: The %run command is used to get variables/ functions that are common for both ingest and transform notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b07cf1e-f1a2-449b-b00b-40050e23544a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./FlightData-commonconfigs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9437254e-d65e-4fab-9aed-b74f2101bc69",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import random\n",
    "import json\n",
    "\n",
    "import os\n",
    "os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\n",
    "import pyspark.pandas as ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cc7d563-f251-45fe-9c0e-9c31fbc49ba1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "####################################### ENVIRONMENT VARIABLES #######################################\n",
    "\n",
    "# Secrets from Azure Key Vault\n",
    "\n",
    "####################################### (JSON) FILES CREATION #######################################\n",
    "\n",
    "url_first_part = dbutils.secrets.get(scope='FlightData-scope', key='url-first-part') \n",
    "url_last_part = dbutils.secrets.get(scope='FlightData-scope', key='url-last-part')\n",
    "user_agent_1 = dbutils.secrets.get(scope='FlightData-scope', key='user-agent-1')\n",
    "user_agent_2 = dbutils.secrets.get(scope='FlightData-scope', key='user-agent-2')\n",
    "user_agent_3 = dbutils.secrets.get(scope='FlightData-scope', key='user-agent-3')\n",
    "user_agent_4 = dbutils.secrets.get(scope='FlightData-scope', key='user-agent-4')\n",
    "user_agent_5 = dbutils.secrets.get(scope='FlightData-scope', key='user-agent-5')\n",
    "user_agent_6 = dbutils.secrets.get(scope='FlightData-scope', key='user-agent-6')\n",
    "user_agent_7 = dbutils.secrets.get(scope='FlightData-scope', key='user-agent-7')\n",
    "\n",
    "var_1_arrivals = dbutils.secrets.get(scope='FlightData-scope', key='var-1-arrivals') \n",
    "var_3_arrivals = dbutils.secrets.get(scope='FlightData-scope', key='var-3-arrivals')\n",
    "\n",
    "var_1_departures = dbutils.secrets.get(scope='FlightData-scope', key='var-1-departures') \n",
    "var_3_departures = dbutils.secrets.get(scope='FlightData-scope', key='var-3-departures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f4d33f3-3df0-4b03-87b5-c4d42e3a5959",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to rotate user agents\n",
    "def header_user_agent():\n",
    "    # List of user-agents to rotate\n",
    "    user_agents = [\n",
    "        user_agent_1,\n",
    "        user_agent_2,\n",
    "        user_agent_3,\n",
    "        user_agent_4,\n",
    "        user_agent_5,\n",
    "        user_agent_6,\n",
    "        user_agent_7\n",
    "    ]\n",
    "    # User Agent request header - using random to rotate user agents\n",
    "    headers = {\"User-Agent\": random.choice(user_agents)}\n",
    "\n",
    "    return headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "add8f638-01af-464d-b540-88a18d17a273",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "####################################### (JSON) FILES CREATION #######################################\n",
    "\n",
    "\n",
    "# Function to create json files for both arrivals and departures - by date\n",
    "def file_creation(var1, var2, var3):\n",
    "\n",
    "    # Filter for only yesterday's flights\n",
    "    # Important to set range of epochs to \"Europe/Malta\" timezone since time data\n",
    "    # from source is in this timezone\n",
    "    epoch_low = int(\n",
    "            ps.to_datetime(date.today() - timedelta(days=1))\n",
    "            .tz_localize(\"Europe/Malta\")\n",
    "            .timestamp()\n",
    "        )\n",
    "    epoch_high = int(\n",
    "        ps.to_datetime(date.today()).tz_localize(\"Europe/Malta\").timestamp()\n",
    "    )\n",
    "\n",
    "    # Empty container - This list will contain all (appended) data\n",
    "    container = []\n",
    "\n",
    "    for i in range(1, 3):\n",
    "        # URL containing arrivals & departures\n",
    "        flight_data_url = f\"{url_first_part}=-{i}&{url_last_part}\"\n",
    "        # get request and set flight_data type to json\n",
    "        headers = header_user_agent()\n",
    "        request = requests.get(url=flight_data_url, headers=headers)\n",
    "        flight_data = request.json()\n",
    "\n",
    "        # Flight data\n",
    "        flights = flight_data[\"result\"][\"response\"][\"airport\"][\"pluginData\"][\n",
    "            \"schedule\"\n",
    "        ][var2][\"data\"]\n",
    "\n",
    "        for flight in flights:\n",
    "            # First check that flight scheduled time is within range - if not, go to next iteration\n",
    "            scheduled_time = flight[\"flight\"][\"time\"][\"scheduled\"][var1]\n",
    "\n",
    "            if scheduled_time < epoch_low or scheduled_time > epoch_high:\n",
    "                continue\n",
    "\n",
    "            # Flight number\n",
    "            if flight[\"flight\"][\"identification\"][\"number\"] is None:\n",
    "                flight_no = None\n",
    "            else:\n",
    "                flight_no = flight[\"flight\"][\"identification\"][\"number\"][\"default\"]\n",
    "            # Airport city\n",
    "            airport_city = flight[\"flight\"][\"airport\"][var3][\"position\"][\"region\"][\n",
    "                \"city\"\n",
    "            ]\n",
    "            # Airport code\n",
    "            airport_code = flight[\"flight\"][\"airport\"][var3][\"code\"][\"iata\"]\n",
    "            # Airline\n",
    "            if flight[\"flight\"][\"airline\"] is None:\n",
    "                airline = None\n",
    "            else:\n",
    "                airline = flight[\"flight\"][\"airline\"][\"name\"]\n",
    "            # Aircraft Type\n",
    "            if flight[\"flight\"][\"aircraft\"] is None:\n",
    "                aircraft_type = None\n",
    "            else:\n",
    "                aircraft_type = flight[\"flight\"][\"aircraft\"][\"model\"][\"text\"]\n",
    "            # Aircraft Registration\n",
    "            if flight[\"flight\"][\"aircraft\"] is None:\n",
    "                aircraft_registration = None\n",
    "            else:\n",
    "                aircraft_registration = flight[\"flight\"][\"aircraft\"][\"registration\"]\n",
    "            # Actual time arrived/departed\n",
    "            actual_time = flight[\"flight\"][\"time\"][\"real\"][var1]\n",
    "\n",
    "            data =  {\n",
    "                \"scheduled_time\": scheduled_time,\n",
    "                \"flight_no\": flight_no,\n",
    "                \"airport_city\": airport_city,\n",
    "                \"airport_code\": airport_code,\n",
    "                \"airline\": airline,\n",
    "                \"aircraft_type\": aircraft_type,\n",
    "                \"aircraft_registration\": aircraft_registration,\n",
    "                \"actual_time\": actual_time,\n",
    "            }\n",
    "\n",
    "            # Appending all flight data to container variable\n",
    "            container.append(data)\n",
    "\n",
    "    # Writing daily flight data to json files for both arrivals and departures \n",
    "    # The JSON files are stored in Databricks dbfs FileStore folder - for previous date\n",
    "    with open(f\"/dbfs/FileStore/flight_data_{var2}_{date.today() - timedelta(days=1)}.json\", \"w\") as json_file:\n",
    "        json.dump(container,json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a802d277-8919-4857-a498-b6201e500348",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "####################################### INGESTION #######################################\n",
    "\n",
    "def ingestion(var2_arrivals,var2_departures,storage_account_name,storage_container_name):\n",
    "\n",
    "    # Defining the schema of the dataframe\n",
    "    schema = StructType([\n",
    "            StructField(\"scheduled_time\", IntegerType(), True), \n",
    "            StructField(\"flight_no\", StringType(), True),\n",
    "            StructField(\"airport_city\", StringType(), True),\n",
    "            StructField(\"airport_code\", StringType(), True),\n",
    "            StructField(\"airline\", StringType(), True),\n",
    "            StructField(\"aircraft_type\", StringType(), True),\n",
    "            StructField(\"aircraft_registration\", StringType(), True),\n",
    "            StructField(\"actual_time\", IntegerType(), True)\n",
    "    ])\n",
    "\n",
    "    # Reading the arrivals JSON files from dbfs FileStore\n",
    "    df1 = spark.read.option(\"multiline\",\"true\") \\\n",
    "        .schema(schema=schema) \\\n",
    "        .json(f\"/FileStore/flight_data_{var2_arrivals}_{date.today() - timedelta(days=1)}.json\")\n",
    "    \n",
    "    df1 = df1.withColumn(\"flight_type\", F.lit('Arrivals'))\n",
    "\n",
    "    # Reading the departures JSON files from dbfs FileStore\n",
    "    df2 = spark.read.option(\"multiline\",\"true\") \\\n",
    "        .schema(schema=schema) \\\n",
    "        .json(f\"/FileStore/flight_data_{var2_departures}_{date.today() - timedelta(days=1)}.json\")\n",
    "    \n",
    "    df2 = df2.withColumn(\"flight_type\", F.lit('Departures'))\n",
    "\n",
    "    # Get the union of both df1 (arrivals) and df2 (departures)\n",
    "    df = df1.union(df2)\n",
    "\n",
    "    # Access mounted ADLS\n",
    "    mount_adls_using_sp(storage_account,storage_ingestion_container)\n",
    "\n",
    "    # Write dataframe to ADLS (as Delta Lake) in 'ADLS Ingestion Layer'\n",
    "    # This will create partitioned folders by date\n",
    "    df.write.format(\"delta\").save(f\"/mnt/{storage_account_name}/{storage_container_name}/{date.today() - timedelta(days=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ca2fac4-b8e8-4164-a7e8-040004235b10",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Driver code\n",
    "\n",
    "####################################### (JSON) FILES CREATION #######################################\n",
    "\n",
    "# Create daily JSON file for arrivals in dbfs FileStore folder\n",
    "file_creation(var_1_arrivals,var_2_arrivals,var_3_arrivals)\n",
    "# Create daily JSON file for departures in dbfs FileStore folder\n",
    "file_creation(var_1_departures, var_2_departures, var_3_departures)\n",
    "\n",
    "####################################### INGESTION #######################################\n",
    "\n",
    "# Ingest data for arrivals & departures in 'ADLS Ingestion Layer' by date\n",
    "ingestion(var_2_arrivals,var_2_departures,storage_account,storage_ingestion_container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc4e3daf-d96c-44dd-9c80-4a43ccfd0f2d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "\n",
    "display(spark.read.format(\"delta\").load(f\"/mnt/{storage_account}/{storage_ingestion_container}/{date.today() - timedelta(days=1)}\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "FlightData-Ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
